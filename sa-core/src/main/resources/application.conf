//// spark conf
//spark {
//  streaming.trigger.time.interval.msec = 1000
//  streaming.future.task.timeout.msec = 300000
//  show.table.numRows = 100
//  show.table.truncate = true
//  redis.cache.data.partition.num = 8
//
//  redis.host = 127.0.0.1
//  redis.port = 6379
//  redis.db = 4
////  redis.auth =
////  redis.timeout =
////  redis.max.pipeline.size =
////  redis.scan.count =
//}
//
//
//sqlalarm {
//  // event sources, can more than one
//  sources = "kafka,redis"
//
//  // alarm event input source conf
//  input {
//    kafka {
//      topic = "sqlalarm_event"
//      subscribe.topic.pattern = 1
//      bootstrap.servers = "127.0.0.1:9092"
//      group = "sqlalarm_group"
//    }
//    redis {
//      keys = "sqlalarm_redis_event"
//      group = "sqlalarm_redis_group"
//      batch.size = 100
//    }
//  }
//
//  // alarm sink, can more than one
//  sinks = "console,kafka,jdbc"
//
//  // alarm record sink canal conf
//  output {
//    kafka {
//
//    }
//    jdbc {
//      url = "jdbc:mysql://127.0.0.1:3306/test?characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false"
//      driver = "com.mysql.jdbc.Driver"
//      user = "xxx"
//      password = "xxx"
//    }
//  }
//
//  checkpointLocation = "checkpoint"
//
//  // alarm alert conf, use rest api usually
//  alert {
//    pigeonApi = "https://dt.sqlclub/api/pigeon"
//  }
//}